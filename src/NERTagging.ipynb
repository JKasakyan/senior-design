{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from basic_nlp import nlp, TWEET_LINK_RE, TWEET_HANDLE_RE\n",
    "\n",
    "from json_io import list_from_json, tweet_map, tweet_iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and setup instructions\n",
    "\n",
    "1) Download zip to [Stanford's Java NER](http://nlp.stanford.edu/software/CRF-NER.shtml#Download)\n",
    "\n",
    "2) Add path to *stanford-ner.jar* to $CLASSPATH environment variable\n",
    "\n",
    "3) For project convenience, set an environment variable *$STANFORD_NER* to path to *stanford-ner-2016-10-31/classifiers/english.all.3class.distsim.crf.ser.gz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check if environment variables set correctly. \n",
    "# Will need to restart shell running 'jupyter notebook' for changes to take effect\n",
    "print(os.environ.get('CLASSPATH'))\n",
    "print(os.environ.get('STANFORD_NER'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = StanfordNERTagger(os.environ.get('STANFORD_NER'), encoding='UTF-8')\n",
    "st.tag('Rami Eid is studying at Stony Brook University in NY'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sarcastic_tweets = list_from_json(\"../json/sarcastic/unique.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ner_tag_tweet(tweet, ner_tagger):\n",
    "    \"\"\"\n",
    "    take a tweet object and a Stanford NER tagger object, and returns tweet object with new field \"ner_text\" \n",
    "    containing words in tweet replaced by entities found by tagger, mentions (@[handle]) replaced by [PERSON],\n",
    "    and embedded links/media replaced by [URL] or [MEDIA] or [URL_MEDIA]\n",
    "    \n",
    "    Stanford NER pre-trained tagger supports up to seven entity types, depending on classifier used:\n",
    "    PERSON (3 class, 4 class, 7 class)\n",
    "    ORGANIZATION (3 class, 4 class, 7 class)\n",
    "    LOCATION (3 class, 4 class, 7 class)\n",
    "    MISCELLANEOUS (4 class)\n",
    "    TIMES (7 class)\n",
    "    MONEY (7 class)\n",
    "    PERCENTS (7 class)\n",
    "    DATES (7 class)\n",
    "    \"\"\"\n",
    "    n = nlp()\n",
    "    \n",
    "    # list of tuples (word, entity) where entity is not \"O\" (tuples where entity is found)\n",
    "    ner_tuples = [ner_tuple for ner_tuple in ner_tagger.tag(n.tokenize(tweet[\"text\"])) if ner_tuple[1] is not \"O\"]\n",
    "    \n",
    "    # construct new string by replacing each word in tweet with its corresponding entity tag\n",
    "    ner_text = tweet[\"text\"] \n",
    "    for word, entity in ner_tuples:\n",
    "        ner_text = ner_text.replace(word, \"[{}]\".format(entity))\n",
    "        \n",
    "    # replace embedded urls/media with [url], [media], or [url_media]\n",
    "    if tweet[\"media\"] or tweet[\"urls\"]:\n",
    "        if tweet['media'] and tweet['urls']:\n",
    "            replacement_word = \"[URL_MEDIA]\"\n",
    "        elif tweet['media']:\n",
    "            replacement_word = \"[MEDIA]\"\n",
    "        else:\n",
    "            replacement_word = \"[URL]\"\n",
    "        \n",
    "        # replace twitter links with appropriate tag\n",
    "        ner_text = re.sub(TWEET_LINK_RE, replacement_word, ner_text)\n",
    "    \n",
    "    # replace mentions (@[handle]) with PERSON tag\n",
    "    ner_text = re.sub(TWEET_HANDLE_RE, \"[PERSON]\", ner_text)\n",
    "    \n",
    "    tweet[\"ner_text\"] = ner_text\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sarcastic_tweet in sarcastic_tweets[:25]:\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(ner_tag_tweet(sarcastic_tweet, st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "for sarcastic_tweet in itertools.islice(tweet_iterate(\"../json/sarcastic/unique.json\"), 25):\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(sarcastic_tweet)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python3 (senior-design)",
   "language": "python",
   "name": "senior-design"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
