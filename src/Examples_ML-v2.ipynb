{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting to remedy space bug by making changes outlined in Space Bug Evaluation notebook and re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/nltk/twitter/__init__.py:20: UserWarning:\n",
      "\n",
      "The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import ml\n",
    "import nlp\n",
    "import json_io\n",
    "\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change 1) repr() not longer called on tweet in feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp.feature??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change 2) Remove trailing spaces before tokenizing (tweet.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp.cleanTokensTwitter??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = ml.JSON_DIR+\"twitter/\"\n",
    "sarcastic_path = path+\"sarcastic/\"\n",
    "serious_path = path+\"serious/\"\n",
    "source = '-twitter-'\n",
    "features_path = 'features/'\n",
    "pickle_path = 'pickled/'\n",
    "\n",
    "n=1\n",
    "\n",
    "\n",
    "test_spaces_sarcastic_text = [\n",
    "             'Having MS is really easy and fun. #sarcasm',\n",
    "             'Having MS is really easy and fun. ',\n",
    "             'Having MS is really easy and fun.',\n",
    "             'Having MS is really easy and fun',\n",
    "             'The classiest event a bar can throw is a foam party. #Sarcasm',\n",
    "             'The classiest event a bar can throw is a foam party. ',\n",
    "             'The classiest event a bar can throw is a foam party.',\n",
    "             'The classiest event a bar can throw is a foam party',\n",
    "             '@thehill Sure I\\'ll watch North Korean - China - Russian and Iran news all very creditable sources #sarcasm',\n",
    "             '@thehill Sure I\\'ll watch North Korean - China - Russian and Iran news all very creditable sources ',\n",
    "             '@thehill Sure I\\'ll watch North Korean - China - Russian and Iran news all very creditable sources',\n",
    "             '@mikefreemanNFL Because voicing your beliefs if they\\'re political is worse than actual physical abuse #sarcasm #idiots',\n",
    "             '@mikefreemanNFL Because voicing your beliefs if they\\'re political is worse than actual physical abuse  #idiots',\n",
    "             '@mikefreemanNFL Because voicing your beliefs if they\\'re political is worse than actual physical abuse #idiots',\n",
    "             '@mikefreemanNFL Because voicing your beliefs if they\\'re political is worse than actual physical abuse #',\n",
    "             '@mikefreemanNFL Because voicing your beliefs if they\\'re political is worse than actual physical abuse ',\n",
    "             '@mikefreemanNFL Because voicing your beliefs if they\\'re political is worse than actual physical abuse',\n",
    "             '@mikefreemanNFL Because voicing your beliefs if they\\'re political is worse than actual physical abuse.',\n",
    "    \n",
    "]\n",
    "\n",
    "test_sarcastic_text = [\n",
    "    \"I'm just curious, but I bet there are thousands of Antifa folks on their way to Texas to help with the Hurricane & Flooding right? #Sarcasm\",\n",
    "    \"More proof that @realDonaldTrump 's base is abandoning him #Sarcasm https://t.co/TQQcvBheDf\",\n",
    "    \"#60MillionStandWithMSG to watch him get what he deserves: adequate conviction and punitive action. #MSG #MSGgreatSocialReformer #Sarcasm\",\n",
    "    \"As someone vying for a career in The Arts, know what I love? Having my career decision constantly questioned by acquaintances #Sarcasm\",\n",
    "    \"@MailOnline wow!! This is a bit harsh what about their human rights ? #sarcasm\",\n",
    "    \"Like Taylor Swift's new video I really need bloggers and journalists to help me decide how I feel about Arsenal. #sarcasm\",\n",
    "    \"Hey, look at thatâ€¦ both Hobby and Intercontinental are closed until further notice. No big deal!! #sarcasm\",\n",
    "    \"Of all the places in Springfield, @POTUS had to pick the building that neighbors my apartment complex for Wednesday. FANTASTIC #sarcasm ðŸ˜’ðŸ™„\",\n",
    "    \"@cogecohelps @cogeco Ok. Cool. So just like the last week of July? And Aug. 12? And last night? That's good. I'll just keep riding these out. #sarcasm\",\n",
    "    \"#lol #funny #comedy #sarcasm me after doing absolutely nothing\",\n",
    "    \"@JeremyMcLellan I thought in Pakistan we did not believe in caste or class system. Apparently, we do!! ðŸ¤£ðŸ¤£ðŸ¤£ At least we can act we don't. ðŸ˜‚ðŸ˜‚ðŸ˜‚#Sarcasm #humor\",\n",
    "    \"@AnaMardoll And that's how we stopped Nazis in the past-- lots of hand-wringing and pleas for peace. #sarcasm\",\n",
    "    \"Don't politicize hurricane #Harvey by saying it's caused by Global Warming, because God is telling us we're living in Last Days. #Sarcasm\",\n",
    "    '''What does a clock do when it's hungry? It goes back four seconds! \n",
    "#DryHumour #Sarcasm #Joke #Funny #HAHAHAHA #MustRead #comedy #Humour''',\n",
    "    \"@UberFacts What?! No way?! I thought we are the most inspired and motivated to do chores when we are feeling depressed, down and lazy as Fk?! #sarcasm\",\n",
    "    \n",
    "]\n",
    "\n",
    "test_spaces_serious_text = [\n",
    "    'Marsch listed Aurelien Collin & Connor Lade as \"day-to-day.\" Keita got a slight knock, didn\\'t train today w/ #RBNY.',\n",
    "    'Marsch listed Aurelien Collin & Connor Lade as \"day-to-day.\" Keita got a slight knock, didn\\'t train today w/ #RBNY. #sarcasm',\n",
    "    'Marsch listed Aurelien Collin & Connor Lade as \"day-to-day.\" Keita got a slight knock, didn\\'t train today w/ #RBNY. ',\n",
    "    'How many retweets to give the Lakers the number 1 pick? @NBA',\n",
    "    'How many retweets to give the Lakers the number 1 pick? @NBA #sarcasm',\n",
    "    'How many retweets to give the Lakers the number 1 pick? @NBA ',\n",
    "    'Never thought that clunkiness/cost of microscope might be holding back public health https://t.co/TQQcvBheDf so $1 microscope exciting!',\n",
    "    'Never thought that clunkiness/cost of microscope might be holding back public health https://t.co/TQQcvBheDf so $1 microscope exciting! #sarcasm',\n",
    "    'Never thought that clunkiness/cost of microscope might be holding back public health https://t.co/TQQcvBheDf so $1 microscope exciting! ',\n",
    "    '@SwiftOnSecurity They sponsor literally every podcast ever, followed by Crunchyroll.',\n",
    "    '@SwiftOnSecurity They sponsor literally every podcast ever, followed by Crunchyroll. #sarcasm',\n",
    "    '@SwiftOnSecurity They sponsor literally every podcast ever, followed by Crunchyroll. ',\n",
    "]\n",
    "\n",
    "test_serious_text = [\n",
    "    '''I think I liked the movie a smidgen less than @melvillmatic did but his is the best piece I've read about Beach Rats https://t.co/TQQcvBheDf''',\n",
    "    '''Take a look at the #PrankItFWD we did with @realtonytiger and 2 aspiring YouTubers! #LetYourGreatOut #Sponsored https://t.co/TQQcvBheDf''',\n",
    "    '''GOT\n",
    "Season 7 exit survey: https://t.co/TQQcvBheDf\n",
    "\n",
    "Finale review: https://t.co/TQQcvBheDf\n",
    "\n",
    "Back to GOT's 'roots': https://t.co/TQQcvBheDf''',\n",
    "    '''I doped like Maria Sharapova and it was actually pretty great: https://t.co/TQQcvBheDf''',\n",
    "    '''@msquinn @DelRey I can't wait for the Kindles + Kale combo package.''',\n",
    "    '''Celtics roster is now at 19 with the addition of LJ Peak. 14 guaranteed, 3 Exhibit 10/Partial and 2 two-way contracts.''',\n",
    "    '''In your opinion, which fight was/is mire one-sided, Mayweather-McGregor, Yankees-Tigers, or Olbermann-sanity?''',\n",
    "    '''Texas needs our help. You can donate money online or text 90999 to chip in $10. If you want to volunteer: https://t.co/TQQcvBheDf''',\n",
    "    '''Thank you so much for your wonderful messages about the TV adaption of #cuckooscalling! Part two airs tonight on @BBCOne at 9pm. https://t.co/TQQcvBheDf''',\n",
    "    '''funny, tillerson doesn't look ðŸŒŽ''',\n",
    "    '''Hey @MalloryRubin @netw3rk can you like auction off the chance to be your friend? I will pay with reimbursement/other.''',\n",
    "    '''Couple who met riding xtown Manhattan bus decided 2 get married while riding xtown Manhattan bus. Next up: the traditional D-train honeymoon''',\n",
    "    '''I want to thank @MalloryRubin  and @netw3rk for guiding me through the journey of Thrones enlightenment. A true pleasure!''',\n",
    "    '''Willy Hernangomez scored a team-high 20 points in Spain's win over Belgium last week. \n",
    "\n",
    "Here are the highlights. \n",
    "Footwork still impressive: https://t.co/TQQcvBheDf''',\n",
    "    '''if you've got a boat in the area please go to this address immediately they've been in need of help since yesterday https://t.co/TQQcvBheDf''',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def results_summary(results):\n",
    "    summary = []\n",
    "    for r in results:\n",
    "        reduction = r[0]\n",
    "        dataset_size = r[1]\n",
    "        highest_score = 0\n",
    "        train_time = 0\n",
    "        for tup in r[2]:\n",
    "            if tup[2] > highest_score:\n",
    "                highest_score = tup[2]\n",
    "                train_time = tup[1]\n",
    "        summary.append({'top_k_feats': reduction, 'dataset_perc': dataset_size, 'train_time': train_time, 'score': highest_score})\n",
    "    return summary\n",
    "        \n",
    "def test_and_print(tweets, classifier, dvp, support=None, y=None):\n",
    "    pre = ml.predict(tweets,\n",
    "           classifier,\n",
    "           dvp,\n",
    "           nlp.cleanTokensTwitter,\n",
    "           support=support)\n",
    "\n",
    "    if y:\n",
    "        correct = 0\n",
    "        total = len(y)\n",
    "        for t,p,pp,tp in zip(tweets, pre['prediction'], pre['prediction_probabilities'], y):\n",
    "            if p == tp:\n",
    "                correct += 1\n",
    "            print(t)\n",
    "            print('\\tSarcastic' if p else '\\tSerious')\n",
    "            print('\\t'+str(pp[1]*100)+'%' if p else '\\t'+str(pp[0]*100)+'%')\n",
    "            print()\n",
    "            \n",
    "        print(\"{} / {} ({}) correct\".format(correct, total, round(correct/total, 2)))\n",
    "    else:\n",
    "        for t,p,pp in zip(tweets, pre['prediction'], pre['prediction_probabilities']):\n",
    "            print(t)\n",
    "            print('\\tSarcastic' if p else '\\tSerious')\n",
    "            print('\\t'+str(pp[1]*100)+'%' if p else '\\t'+str(pp[0]*100)+'%')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process tweets and fit transform features with DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, dvp = ml.process_X_y_dvp(sarcastic_path, serious_path, save=True, X_fn='testX.pickle', y_fn='testY.pickle', dvp_fn='testDVP.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optionally load from pickle if saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pickle.load(open(pickle_path + 'v2-twitter-X-unbalanced.pickle', 'rb'))\n",
    "y = pickle.load(open(pickle_path + 'v2-twitter-y-unbalanced.pickle', 'rb'))\n",
    "dvp = pickle.load(open(pickle_path + 'v2-twitter-dvp.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(dvp.get_feature_names()))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a balanced X, y training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_bal, y_bal = ml.even_samples(X,y,len(y[y==True]))\n",
    "pickle.dump(X_bal, open(pickle_path + \"v2-twitter-X-balanced.pickle\", 'wb'))\n",
    "pickle.dump(y_bal, open(pickle_path + \"v2-twitter-y-balanced.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optionally load from pickle if saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_bal = pickle.load(open(pickle_path + 'v2-twitter-X-balanced.pickle', 'rb'))\n",
    "y_bal = pickle.load(open(pickle_path + 'v2-twitter-y-balanced.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown of unbalanced vs balanced training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_sarc = len(y[y==True])\n",
    "total_ser = len(y[y==False])\n",
    "total = total_sarc + total_ser\n",
    "serious_percent = total_ser / total\n",
    "sarcastic_percent = total_sarc / total\n",
    "\n",
    "total_sarc_bal = len(y_bal[y_bal==True])\n",
    "total_ser_bal = len(y_bal[y_bal==False])\n",
    "total_bal = total_sarc_bal + total_ser_bal\n",
    "serious_percent_bal = total_ser_bal / total_bal\n",
    "sarcastic_percent_bal = total_sarc_bal / total_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"8b3d4ec6-8fce-4d8e-ab22-77c92a2688b4\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"8b3d4ec6-8fce-4d8e-ab22-77c92a2688b4\", [{\"domain\": {\"x\": [0, 0.48]}, \"textinfo\": \"percent+value\", \"labels\": [\"Sarcastic\", \"Serious\"], \"values\": [52679, 163297], \"type\": \"pie\"}, {\"domain\": {\"x\": [0.52, 1]}, \"textinfo\": \"percent+value\", \"labels\": [\"Sarcastic\", \"Serious\"], \"values\": [52679, 52679], \"type\": \"pie\"}], {\"annotations\": [{\"font\": {\"size\": 14}, \"showarrow\": false, \"text\": \"Unbalanced\", \"y\": 1.1, \"x\": 0.17}, {\"font\": {\"size\": 14}, \"showarrow\": false, \"text\": \"Balanced\", \"y\": 1.1, \"x\": 0.81}], \"title\": \"Distribution of twitter dataset\"}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['Sarcastic','Serious']\n",
    "values = [total_sarc, total_ser]\n",
    "values_bal = [total_sarc_bal, total_ser_bal]\n",
    "\n",
    "fig = {\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"values\": values,\n",
    "            \"labels\": labels,\n",
    "            \"textinfo\": \"percent+value\",\n",
    "            \"type\": \"pie\",\n",
    "            \"domain\": {\"x\": [0, .48]}\n",
    "        },\n",
    "        {\n",
    "            \"values\": values_bal,\n",
    "            \"labels\": labels,\n",
    "            \"textinfo\": \"percent+value\",\n",
    "            \"type\": \"pie\",\n",
    "            \"domain\": {\"x\": [.52, 1]}\n",
    "        }],\n",
    "    \"layout\": {\n",
    "        \"title\": \"Distribution of twitter dataset\",\n",
    "        \"annotations\": [\n",
    "            {\n",
    "                \"font\": {\n",
    "                    \"size\": 14\n",
    "                },\n",
    "                \"showarrow\": False,\n",
    "                \"text\": \"Unbalanced\",\n",
    "                \"x\": 0.17,\n",
    "                \"y\": 1.10\n",
    "            },\n",
    "            {\n",
    "                \"font\": {\n",
    "                    \"size\": 14\n",
    "                },\n",
    "                \"showarrow\": False,\n",
    "                \"text\": \"Balanced\",\n",
    "                \"x\": 0.81,\n",
    "                \"y\": 1.10\n",
    "            }]\n",
    "    }\n",
    "\n",
    "        \n",
    "}\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test, report and save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unbalanced Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml.trainTestMultiple(X, \n",
    "                     y, \n",
    "                     reduce_amounts=[0, 1000000, 500000, 100000, 50000, 10000, 5000, 1000, 500, 100, 25, 10, 2, 1],\n",
    "                     train_sizes=[0.8],\n",
    "                     classifiers=[MultinomialNB()],\n",
    "                     splits=5,\n",
    "                     save=True,\n",
    "                     results_fn = pickle_path + 'v2-twitter-trained-unbalanced.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unbalanced Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tReduction: 0\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 503\tScore:\t0.832971\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 365\tScore:\t0.829359\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 299\tScore:\t0.832160\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 325\tScore:\t0.833110\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 330\tScore:\t0.833248\n",
      "\n",
      "\t\tReduction: 1000000\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Samples, Features before reduction: (172780, 2821370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning:\n",
      "\n",
      "Features [0 0 0 ..., 0 0 0] are constant.\n",
      "\n",
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples, Features after reduction: (172780, 1000000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 234\tScore:\t0.831281\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 1000000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 230\tScore:\t0.831489\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 1000000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 226\tScore:\t0.831651\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 1000000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 228\tScore:\t0.831211\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 1000000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 231\tScore:\t0.832554\n",
      "\n",
      "\t\tReduction: 500000\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Samples, Features before reduction: (172780, 2821370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning:\n",
      "\n",
      "Features [0 0 0 ..., 0 0 0] are constant.\n",
      "\n",
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples, Features after reduction: (172780, 500000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 172\tScore:\t0.830494\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 500000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 182\tScore:\t0.830540\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 500000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 193\tScore:\t0.832508\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 500000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 179\tScore:\t0.827993\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 500000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 169\tScore:\t0.831373\n",
      "\n",
      "\t\tReduction: 100000\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Samples, Features before reduction: (172780, 2821370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning:\n",
      "\n",
      "Features [0 0 0 ..., 0 0 0] are constant.\n",
      "\n",
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples, Features after reduction: (172780, 100000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 168\tScore:\t0.831558\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 100000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 169\tScore:\t0.827692\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 100000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 175\tScore:\t0.827947\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 100000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 188\tScore:\t0.829799\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 100000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 176\tScore:\t0.829706\n",
      "\n",
      "\t\tReduction: 50000\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Samples, Features before reduction: (172780, 2821370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning:\n",
      "\n",
      "Features [0 0 0 ..., 0 0 0] are constant.\n",
      "\n",
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples, Features after reduction: (172780, 50000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 139\tScore:\t0.827160\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 50000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 157\tScore:\t0.828109\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 50000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 171\tScore:\t0.827553\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 50000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 168\tScore:\t0.828364\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 50000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 139\tScore:\t0.828873\n",
      "\n",
      "\t\tReduction: 10000\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Samples, Features before reduction: (172780, 2821370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning:\n",
      "\n",
      "Features [0 0 0 ..., 0 0 0] are constant.\n",
      "\n",
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples, Features after reduction: (172780, 10000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 133\tScore:\t0.824081\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 10000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 186\tScore:\t0.825447\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 10000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 123\tScore:\t0.826095\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 10000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 126\tScore:\t0.824752\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 10000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 128\tScore:\t0.827183\n",
      "\n",
      "\t\tReduction: 5000\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Samples, Features before reduction: (172780, 2821370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning:\n",
      "\n",
      "Features [0 0 0 ..., 0 0 0] are constant.\n",
      "\n",
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples, Features after reduction: (172780, 5000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 137\tScore:\t0.820678\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 5000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 101\tScore:\t0.821257\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 5000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 149\tScore:\t0.820863\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 5000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 143\tScore:\t0.822159\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 5000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 152\tScore:\t0.822298\n",
      "\n",
      "\t\tReduction: 1000\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Samples, Features before reduction: (172780, 2821370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning:\n",
      "\n",
      "Features [0 0 0 ..., 0 0 0] are constant.\n",
      "\n",
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples, Features after reduction: (172780, 1000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 47\tScore:\t0.797620\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 1000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 75\tScore:\t0.798639\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 1000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 50\tScore:\t0.799310\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 1000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 54\tScore:\t0.800560\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 1000)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 51\tScore:\t0.799079\n",
      "\n",
      "\t\tReduction: 500\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Samples, Features before reduction: (172780, 2821370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning:\n",
      "\n",
      "Features [0 0 0 ..., 0 0 0] are constant.\n",
      "\n",
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples, Features after reduction: (172780, 500)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 34\tScore:\t0.788383\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 500)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 29\tScore:\t0.787480\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 500)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 42\tScore:\t0.787642\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 500)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 38\tScore:\t0.786462\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 500)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 30\tScore:\t0.785466\n",
      "\n",
      "\t\tReduction: 100\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Samples, Features before reduction: (172780, 2821370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning:\n",
      "\n",
      "Features [0 0 0 ..., 0 0 0] are constant.\n",
      "\n",
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples, Features after reduction: (172780, 100)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 10\tScore:\t0.761876\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 100)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 13\tScore:\t0.760788\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 100)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 15\tScore:\t0.761367\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 100)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 15\tScore:\t0.761807\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 100)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 15\tScore:\t0.764145\n",
      "\n",
      "\t\tReduction: 25\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Samples, Features before reduction: (172780, 2821370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning:\n",
      "\n",
      "Features [0 0 0 ..., 0 0 0] are constant.\n",
      "\n",
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples, Features after reduction: (172780, 25)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 3\tScore:\t0.752523\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 25)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 3\tScore:\t0.754838\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 25)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 3\tScore:\t0.753588\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 25)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 3\tScore:\t0.754491\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 25)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 2\tScore:\t0.752222\n",
      "\n",
      "\t\tReduction: 10\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Samples, Features before reduction: (172780, 2821370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning:\n",
      "\n",
      "Features [0 0 0 ..., 0 0 0] are constant.\n",
      "\n",
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples, Features after reduction: (172780, 10)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 1\tScore:\t0.750556\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 10)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.750046\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 10)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.751759\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 10)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.752384\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 10)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.750857\n",
      "\n",
      "\t\tReduction: 2\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Samples, Features before reduction: (172780, 2821370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning:\n",
      "\n",
      "Features [0 0 0 ..., 0 0 0] are constant.\n",
      "\n",
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples, Features after reduction: (172780, 2)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.752940\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 2)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.753310\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 2)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.753056\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 2)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.753611\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 2)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.752986\n",
      "\n",
      "\t\tReduction: 1\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Samples, Features before reduction: (172780, 2821370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning:\n",
      "\n",
      "Features [0 0 0 ..., 0 0 0] are constant.\n",
      "\n",
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in true_divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples, Features after reduction: (172780, 1)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.749884\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 1)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.753681\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 1)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.749745\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 1)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.750764\n",
      "Samples, Features before reduction: (172780, 2821370)\n",
      "Samples, Features after reduction: (172780, 1)\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.750440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  0.8,\n",
       "  [(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    503.614454,\n",
       "    0.83297064543013244),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False), 365.248968, 0.82935919992591911),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False), 299.547353, 0.83216038522085378),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False), 325.26462, 0.83310954718029451),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False), 330.759104, 0.83324844893045658)]),\n",
       " (1000000,\n",
       "  0.8,\n",
       "  [(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    234.976101,\n",
       "    0.83128067413649409,\n",
       "    array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    230.261061,\n",
       "    0.83148902676173719,\n",
       "    array([ True,  True,  True, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    226.492774,\n",
       "    0.8316510788035929,\n",
       "    array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    228.836345,\n",
       "    0.83121122326141306,\n",
       "    array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    231.06921,\n",
       "    0.83255394017964623,\n",
       "    array([ True,  True,  True, ..., False, False, False], dtype=bool))]),\n",
       " (500000,\n",
       "  0.8,\n",
       "  [(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    172.638499,\n",
       "    0.83049356421890919,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    182.146032,\n",
       "    0.83053986480229647,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    193.96635,\n",
       "    0.83250763959625895,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    179.812516,\n",
       "    0.82799333271599218,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    169.816564,\n",
       "    0.83137327530326877,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool))]),\n",
       " (100000,\n",
       "  0.8,\n",
       "  [(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    168.828782,\n",
       "    0.83155847763681823,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    169.056452,\n",
       "    0.8276923789239744,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    175.183999,\n",
       "    0.8279470321326049,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    188.290923,\n",
       "    0.82979905546809885,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    176.91159,\n",
       "    0.82970645430132417,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool))]),\n",
       " (50000,\n",
       "  0.8,\n",
       "  [(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    139.713526,\n",
       "    0.82715992221501988,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    157.054758,\n",
       "    0.82810908417446061,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    171.154615,\n",
       "    0.82755347717381234,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    168.874787,\n",
       "    0.82836373738309099,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    139.360592,\n",
       "    0.82887304380035187,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool))]),\n",
       " (10000,\n",
       "  0.8,\n",
       "  [(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    133.540225,\n",
       "    0.82408093341976107,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    186.34079,\n",
       "    0.82544680062968789,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    123.547849,\n",
       "    0.82609500879711084,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    126.568325,\n",
       "    0.82475229187887766,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    128.704334,\n",
       "    0.82718307250671363,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool))]),\n",
       " (5000,\n",
       "  0.8,\n",
       "  [(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    137.235869,\n",
       "    0.82067784054079085,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    101.825008,\n",
       "    0.82125659783313265,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    149.457484,\n",
       "    0.8208630428743402,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    143.930004,\n",
       "    0.82215945920918598,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    152.066445,\n",
       "    0.82229836095934805,\n",
       "    array([ True,  True, False, ..., False, False, False], dtype=bool))]),\n",
       " (1000,\n",
       "  0.8,\n",
       "  [(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    47.194025,\n",
       "    0.79762015001389019,\n",
       "    array([ True, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    75.767283,\n",
       "    0.79863876284841184,\n",
       "    array([ True, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    50.572013,\n",
       "    0.79931012130752843,\n",
       "    array([ True, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    54.348692,\n",
       "    0.80056023705898693,\n",
       "    array([ True, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    51.171312,\n",
       "    0.79907861839059169,\n",
       "    array([ True, False, False, ..., False, False, False], dtype=bool))]),\n",
       " (500,\n",
       "  0.8,\n",
       "  [(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    34.813741,\n",
       "    0.78838318362811366,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    29.458032,\n",
       "    0.78748032225206033,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    42.825178,\n",
       "    0.78764237429391615,\n",
       "    array([ True, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    38.296649,\n",
       "    0.78646170941753868,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    30.013482,\n",
       "    0.78546624687471067,\n",
       "    array([ True, False, False, ..., False, False, False], dtype=bool))]),\n",
       " (100,\n",
       "  0.8,\n",
       "  [(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    10.366504,\n",
       "    0.7618760996388555,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    13.562661,\n",
       "    0.7607880359292527,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    15.358224,\n",
       "    0.76136679322159462,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    15.547854,\n",
       "    0.76180664876377446,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    15.904113,\n",
       "    0.76414482822483565,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool))]),\n",
       " (25,\n",
       "  0.8,\n",
       "  [(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    3.316464,\n",
       "    0.75252338179461065,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    3.014735,\n",
       "    0.75483841096397819,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    3.904359,\n",
       "    0.75358829521251969,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    3.870648,\n",
       "    0.75449115658857302,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    2.860931,\n",
       "    0.75222242800259287,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool))]),\n",
       " (10,\n",
       "  0.8,\n",
       "  [(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    1.225999,\n",
       "    0.75055560700064816,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    0.922875,\n",
       "    0.75004630058338739,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    0.86173,\n",
       "    0.75175942216871927,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    0.856435,\n",
       "    0.75238448004444858,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    0.888653,\n",
       "    0.75085656079266594,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool))]),\n",
       " (2,\n",
       "  0.8,\n",
       "  [(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    0.218351,\n",
       "    0.75294008704509674,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    0.235053,\n",
       "    0.75331049171219555,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    0.231373,\n",
       "    0.75305583850356517,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    0.222873,\n",
       "    0.75361144550421333,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    0.214842,\n",
       "    0.75298638762848413,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool))]),\n",
       " (1,\n",
       "  0.8,\n",
       "  [(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    0.17432,\n",
       "    0.74988424854153157,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    0.181347,\n",
       "    0.75368089637929436,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    0.175773,\n",
       "    0.74974534679136962,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    0.169876,\n",
       "    0.75076395962589126,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool)),\n",
       "   (LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    0.171914,\n",
       "    0.75043985554217985,\n",
       "    array([False, False, False, ..., False, False, False], dtype=bool))])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.trainTestMultiple(X, \n",
    "                     y, \n",
    "                     reduce_amounts=[0, 1000000, 500000, 100000, 50000, 10000, 5000, 1000, 500, 100, 25, 10, 2, 1],\n",
    "                     train_sizes=[0.8],\n",
    "                     classifiers=[LogisticRegression(n_jobs=1)],\n",
    "                     splits=5,\n",
    "                     save=True,\n",
    "                     results_fn = pickle_path + 'v2-twitter-trained-log-unbalanced.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml.trainTestMultiple(X_bal, \n",
    "                     y_bal, \n",
    "                     reduce_amounts=[0, 1000000, 500000, 100000, 50000, 10000, 5000, 1000, 500, 100, 25, 10, 2, 1],\n",
    "                     train_sizes=[0.8],\n",
    "                     classifiers=[MultinomialNB()],\n",
    "                     splits=5,\n",
    "                     save=True,\n",
    "                     results_fn = pickle_path + 'v2-twitter-trained-balanced.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml.trainTestMultiple(X_bal, \n",
    "                     y_bal, \n",
    "                     reduce_amounts=[0, 1000000, 500000, 100000, 50000, 10000, 5000, 1000, 500, 100, 25, 10, 2, 1],\n",
    "                     train_sizes=[0.8],\n",
    "                     classifiers=[LogisticRegression(n_jobs=1)],\n",
    "                     splits=5,\n",
    "                     save=True,\n",
    "                     results_fn = pickle_path + 'v2-twitter-trained-log-balanced.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best classifier from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvp = pickle.load(open(pickle_path + \"v2-twitter-dvp.pickle\", 'rb'))\n",
    "results_NB_unbal = pickle.load(open(pickle_path + 'v2-twitter-trained-unbalanced.pickle', 'rb'))\n",
    "results_NB_bal = pickle.load(open(pickle_path + 'v2-twitter-trained-balanced.pickle', 'rb'))\n",
    "results_LOG_unbal = pickle.load(open(pickle_path + 'v2-twitter-trained-log-unbalanced.pickle', 'rb'))\n",
    "results_LOG_bal = pickle.load(open(pickle_path + 'v2-twitter-trained-log-balanced.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_classifiers = ml.best_classifiers([results_NB_unbal, results_NB_bal, results_LOG_unbal, results_LOG_bal],\n",
    "                   ['NB_unbal', 'NB_bal', 'LOG_unbal', 'LOG_bal'],\n",
    "                   dvp,\n",
    "                   save=True,\n",
    "                   fn=pickle_path + 'best.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optionally load from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'LOG_bal': {'classifier': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                        intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "                        penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "                        verbose=0, warm_start=False),\n",
       "              'dvp': DictVectorizerPartial(dtype=<class 'numpy.float32'>, feature_names=None,\n",
       "                         separator='=', sparse=True, vocab=None),\n",
       "              'reduction': 0,\n",
       "              'score': 0.7630979498861048,\n",
       "              'size': 0.8,\n",
       "              'train_time': 127.964023},\n",
       "             'LOG_unbal': {'classifier': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                        intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "                        penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "                        verbose=0, warm_start=False),\n",
       "              'dvp': DictVectorizerPartial(dtype=<class 'numpy.float32'>, feature_names=None,\n",
       "                         separator='=', sparse=True, vocab=None),\n",
       "              'reduction': 1000000,\n",
       "              'score': 0.83424391147328458,\n",
       "              'size': 0.8,\n",
       "              'train_time': 261.859573},\n",
       "             'NB_bal': {'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "              'dvp': DictVectorizerPartial(dtype=<class 'numpy.float32'>, feature_names=None,\n",
       "                         separator='=', sparse=True, vocab=None),\n",
       "              'reduction': 100000,\n",
       "              'score': 0.74250189825360668,\n",
       "              'size': 0.8,\n",
       "              'train_time': 0.169391},\n",
       "             'NB_unbal': {'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "              'dvp': DictVectorizerPartial(dtype=<class 'numpy.float32'>, feature_names=None,\n",
       "                         separator='=', sparse=True, vocab=None),\n",
       "              'reduction': 0,\n",
       "              'score': 0.80875544031854807,\n",
       "              'size': 0.8,\n",
       "              'train_time': 0.897945}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifiers = pickle.load(open(pickle_path + 'best.pickle', 'rb'))\n",
    "best_classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best unbalanced Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best classifier: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "Best score: 0.81\n",
      "Feature reduction:  top 0\n",
      "Size reduction: 80.0% of dataset\n",
      "Training time: 0.9s\n",
      "***********\n",
      "Top 5 features indicating sarcasm:\n",
      "Sarcastic vow1 1 -3.13071281306\n",
      "Sarcastic vow2 1 1 -3.39840588209\n",
      "Sarcastic vow3 1 1 1 -3.64514280255\n",
      "Sarcastic syl1 1 -3.71395637172\n",
      "Sarcastic vow4 1 1 1 1 -3.89685803047\n",
      "Top 5 features indicating serious:\n",
      "Serious \\_RAW -16.2749198706\n",
      "Serious \\_RAW/LEN -16.2749198706\n",
      "Serious \\_RAW/TOTAL_PUNCT_FOUND -16.2749198706\n",
      "Serious grm1 ''A -16.2749198706\n",
      "Serious grm1 ''Come -16.2749198706\n"
     ]
    }
   ],
   "source": [
    "nb_unbal = best_classifiers['NB_unbal']\n",
    "\n",
    "print(\"Best classifier: {}\".format(nb_unbal['classifier']))\n",
    "print(\"Best score: {}\".format(round(nb_unbal['score'], 2)))\n",
    "print(\"Feature reduction:  top {}\".format(nb_unbal['reduction']))\n",
    "print(\"Size reduction: {}% of dataset\".format(nb_unbal['size']*100))\n",
    "print(\"Training time: {}s\".format(round(nb_unbal['train_time'], 2)))\n",
    "print(\"***********\")\n",
    "print(\"Top 5 features indicating sarcasm:\")\n",
    "for label, feat, coef in ml.top_n_features(nb_unbal['classifier'], nb_unbal['dvp'], True, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)\n",
    "print(\"Top 5 features indicating serious:\")\n",
    "for label, feat, coef in ml.top_n_features(nb_unbal['classifier'], nb_unbal['dvp'], False, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best balanced Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_bal = best_classifiers['NB_bal']\n",
    "\n",
    "print(\"Best classifier: {}\".format(nb_bal['classifier']))\n",
    "print(\"Best score: {}\".format(round(nb_bal['score'], 2)))\n",
    "print(\"Feature reduction:  top {}\".format(nb_bal['reduction']))\n",
    "print(\"Size reduction: {}% of dataset\".format(nb_bal['size']*100))\n",
    "print(\"Training time: {}s\".format(round(nb_bal['train_time'], 2)))\n",
    "print(\"***********\")\n",
    "print(\"Top 5 features indicating sarcasm:\")\n",
    "for label, feat, coef in ml.top_n_features(nb_bal['classifier'], nb_bal['dvp'], True, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)\n",
    "\n",
    "print(\"Top 5 features indicating serious:\")\n",
    "for label, feat, coef in ml.top_n_features(nb_bal['classifier'], nb_bal['dvp'], False, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best unbalanced Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_unbal = best_classifiers['LOG_unbal']\n",
    "\n",
    "print(\"Best classifier: {}\".format(log_unbal['classifier']))\n",
    "print(\"Best score: {}\".format(round(log_unbal['score'], 2)))\n",
    "print(\"Feature reduction:  top {}\".format(log_unbal['reduction']))\n",
    "print(\"Size reduction: {}% of dataset\".format(log_unbal['size']*100))\n",
    "print(\"Training time: {}s\".format(round(log_unbal['train_time'], 2)))\n",
    "print(\"***********\")\n",
    "print(\"Top 5 features indicating sarcasm:\")\n",
    "for label, feat, coef in ml.top_n_features(log_unbal['classifier'], log_unbal['dvp'], True, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)\n",
    "\n",
    "print(\"Top 5 features indicating serious:\")\n",
    "for label, feat, coef in ml.top_n_features(log_unbal['classifier'], log_unbal['dvp'], False, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best balanced Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best classifier: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Best score: 0.76\n",
      "Feature reduction:  top 0\n",
      "Size reduction: 80.0% of dataset\n",
      "Training time: 127.96s\n",
      "***********\n",
      "Top 5 features indicating sarcasm:\n",
      "Sarcastic grm1 â€¦ 1.1892129844\n",
      "Sarcastic grm1 sarcasm 0.991065785677\n",
      "Sarcastic :_RAW/TOTAL_PUNCT_FOUND 0.904427486998\n",
      "Sarcastic grm1 shocked 0.809807444132\n",
      "Sarcastic grm1 Because 0.750912049011\n",
      "Top 5 features indicating serious:\n",
      "Serious grm1 sex -0.703245624411\n",
      "Serious grm1 video -0.70145052939\n",
      "Serious grm2 . - -0.667473039543\n",
      "Serious grm1 porn -0.5910462022\n",
      "Serious grm1 shit -0.563033171018\n"
     ]
    }
   ],
   "source": [
    "log_bal = best_classifiers['LOG_bal']\n",
    "\n",
    "print(\"Best classifier: {}\".format(log_bal['classifier']))\n",
    "print(\"Best score: {}\".format(round(log_bal['score'], 2)))\n",
    "print(\"Feature reduction:  top {}\".format(log_bal['reduction']))\n",
    "print(\"Size reduction: {}% of dataset\".format(log_bal['size']*100))\n",
    "print(\"Training time: {}s\".format(round(log_bal['train_time'], 2)))\n",
    "print(\"***********\")\n",
    "print(\"Top 5 features indicating sarcasm:\")\n",
    "for label, feat, coef in ml.top_n_features(log_bal['classifier'], log_bal['dvp'], True, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)\n",
    "\n",
    "print(\"Top 5 features indicating serious:\")\n",
    "for label, feat, coef in ml.top_n_features(log_bal['classifier'], log_bal['dvp'], False, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = [results_NB_unbal, results_NB_bal, results_LOG_unbal, results_LOG_bal]\n",
    "summaries = [results_summary(r) for r in results]\n",
    "names = [\"Unbalanced NB\", \"Balanced NB\", \"Unbalanced Logistic\", \"Balanced Logistic\"]\n",
    "top_k_feats = np.array([d['top_k_feats'] for d in summaries[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"f6bb21fc-9b22-468a-a070-3c111e91c52e\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"f6bb21fc-9b22-468a-a070-3c111e91c52e\", [{\"name\": \"Unbalanced NB\", \"type\": \"scatter\", \"x\": [25, 10, 2, 1], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"red\"}, \"y\": [0.7247430317622002, 0.7333086396888601, 0.7560885267154366, 0.7560885267154366], \"text\": [\"Feature reduction: 25, Score: 0.7247430317622002\", \"Feature reduction: 10, Score: 0.7333086396888601\", \"Feature reduction: 2, Score: 0.7560885267154366\", \"Feature reduction: 1, Score: 0.7560885267154366\"], \"mode\": \"markers\"}, {\"name\": \"Balanced NB\", \"type\": \"scatter\", \"x\": [25, 10, 2, 1], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"green\"}, \"y\": [0.6072987851176918, 0.6100037965072134, 0.6023158694001519, 0.5], \"text\": [\"Feature reduction: 25, Score: 0.6072987851176918\", \"Feature reduction: 10, Score: 0.6100037965072134\", \"Feature reduction: 2, Score: 0.6023158694001519\", \"Feature reduction: 1, Score: 0.5\"], \"mode\": \"markers\"}, {\"name\": \"Unbalanced Logistic\", \"type\": \"scatter\", \"x\": [25, 10, 2, 1], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"blue\"}, \"y\": [0.755347717381239, 0.7510880637096028, 0.7540050004630058, 0.7537966478377628], \"text\": [\"Feature reduction: 25, Score: 0.755347717381239\", \"Feature reduction: 10, Score: 0.7510880637096028\", \"Feature reduction: 2, Score: 0.7540050004630058\", \"Feature reduction: 1, Score: 0.7537966478377628\"], \"mode\": \"markers\"}, {\"name\": \"Balanced Logistic\", \"type\": \"scatter\", \"x\": [25, 10, 2, 1], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"yellow\"}, \"y\": [0.6611142748671223, 0.6421317388003037, 0.6363895216400911, 0.6388097949886105], \"text\": [\"Feature reduction: 25, Score: 0.6611142748671223\", \"Feature reduction: 10, Score: 0.6421317388003037\", \"Feature reduction: 2, Score: 0.6363895216400911\", \"Feature reduction: 1, Score: 0.6388097949886105\"], \"mode\": \"markers\"}], {\"yaxis\": {\"titlefont\": {\"size\": 10}, \"title\": \"Score\"}, \"xaxis\": {\"titlefont\": {\"size\": 10}, \"title\": \"Feature reduction (top k)\"}, \"title\": \"Tweet classifiers  5-fold CV training error (total features: 2821370, full dataset size: 215976, 80-20 train test split)\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"0f3a50d7-d900-40b6-8f05-f8d7226d90e6\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"0f3a50d7-d900-40b6-8f05-f8d7226d90e6\", [{\"name\": \"Unbalanced NB\", \"type\": \"scatter\", \"x\": [5000, 1000, 500, 100], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"red\"}, \"y\": [0.7326141309380498, 0.7225900546346884, 0.7275905176405223, 0.7207148810075007], \"text\": [\"Feature reduction: 5000, Score: 0.7326141309380498\", \"Feature reduction: 1000, Score: 0.7225900546346884\", \"Feature reduction: 500, Score: 0.7275905176405223\", \"Feature reduction: 100, Score: 0.7207148810075007\"], \"mode\": \"markers\"}, {\"name\": \"Balanced NB\", \"type\": \"scatter\", \"x\": [5000, 1000, 500, 100], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"green\"}, \"y\": [0.7154043280182233, 0.6828018223234624, 0.6714597570235383, 0.6346336370539104], \"text\": [\"Feature reduction: 5000, Score: 0.7154043280182233\", \"Feature reduction: 1000, Score: 0.6828018223234624\", \"Feature reduction: 500, Score: 0.6714597570235383\", \"Feature reduction: 100, Score: 0.6346336370539104\"], \"mode\": \"markers\"}, {\"name\": \"Unbalanced Logistic\", \"type\": \"scatter\", \"x\": [5000, 1000, 500, 100], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"blue\"}, \"y\": [0.8227150662098343, 0.8018566533938327, 0.7872719696268173, 0.7632882674321696], \"text\": [\"Feature reduction: 5000, Score: 0.8227150662098343\", \"Feature reduction: 1000, Score: 0.8018566533938327\", \"Feature reduction: 500, Score: 0.7872719696268173\", \"Feature reduction: 100, Score: 0.7632882674321696\"], \"mode\": \"markers\"}, {\"name\": \"Balanced Logistic\", \"type\": \"scatter\", \"x\": [5000, 1000, 500, 100], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"yellow\"}, \"y\": [0.7481966590736523, 0.7221431283219438, 0.7014047076689446, 0.6773917995444191], \"text\": [\"Feature reduction: 5000, Score: 0.7481966590736523\", \"Feature reduction: 1000, Score: 0.7221431283219438\", \"Feature reduction: 500, Score: 0.7014047076689446\", \"Feature reduction: 100, Score: 0.6773917995444191\"], \"mode\": \"markers\"}], {\"yaxis\": {\"titlefont\": {\"size\": 10}, \"title\": \"Score\"}, \"xaxis\": {\"titlefont\": {\"size\": 10}, \"title\": \"Feature reduction (top k)\"}, \"title\": \"Tweet classifiers  5-fold CV training error (total features: 2821370, full dataset size: 215976, 80-20 train test split)\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"5c4d5467-a34d-4247-8dfa-2146e801e675\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5c4d5467-a34d-4247-8dfa-2146e801e675\", [{\"name\": \"Unbalanced NB\", \"type\": \"scatter\", \"x\": [0, 1000000, 500000, 100000, 50000, 10000], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"red\"}, \"y\": [0.8087554403185481, 0.8065793128993425, 0.8063015093990185, 0.7807667376608945, 0.7648161866839522, 0.7433790165756089], \"text\": [\"Feature reduction: 0, Score: 0.8087554403185481\", \"Feature reduction: 1000000, Score: 0.8065793128993425\", \"Feature reduction: 500000, Score: 0.8063015093990185\", \"Feature reduction: 100000, Score: 0.7807667376608945\", \"Feature reduction: 50000, Score: 0.7648161866839522\", \"Feature reduction: 10000, Score: 0.7433790165756089\"], \"mode\": \"markers\"}, {\"name\": \"Balanced NB\", \"type\": \"scatter\", \"x\": [0, 1000000, 500000, 100000, 50000, 10000], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"green\"}, \"y\": [0.7155941533788914, 0.7115603644646925, 0.7085706150341685, 0.7425018982536067, 0.738705391040243, 0.7220482156416097], \"text\": [\"Feature reduction: 0, Score: 0.7155941533788914\", \"Feature reduction: 1000000, Score: 0.7115603644646925\", \"Feature reduction: 500000, Score: 0.7085706150341685\", \"Feature reduction: 100000, Score: 0.7425018982536067\", \"Feature reduction: 50000, Score: 0.738705391040243\", \"Feature reduction: 10000, Score: 0.7220482156416097\"], \"mode\": \"markers\"}, {\"name\": \"Unbalanced Logistic\", \"type\": \"scatter\", \"x\": [0, 1000000, 500000, 100000, 50000, 10000], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"blue\"}, \"y\": [0.8336651541809427, 0.8342439114732846, 0.8326465413464209, 0.8307713677192332, 0.8287109917584962, 0.8270441707565516], \"text\": [\"Feature reduction: 0, Score: 0.8336651541809427\", \"Feature reduction: 1000000, Score: 0.8342439114732846\", \"Feature reduction: 500000, Score: 0.8326465413464209\", \"Feature reduction: 100000, Score: 0.8307713677192332\", \"Feature reduction: 50000, Score: 0.8287109917584962\", \"Feature reduction: 10000, Score: 0.8270441707565516\"], \"mode\": \"markers\"}, {\"name\": \"Balanced Logistic\", \"type\": \"scatter\", \"x\": [0, 1000000, 500000, 100000, 50000, 10000], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"yellow\"}, \"y\": [0.7630979498861048, 0.7601082004555809, 0.7572608200455581, 0.7570709946848899, 0.7580201214882308, 0.7559320425208808], \"text\": [\"Feature reduction: 0, Score: 0.7630979498861048\", \"Feature reduction: 1000000, Score: 0.7601082004555809\", \"Feature reduction: 500000, Score: 0.7572608200455581\", \"Feature reduction: 100000, Score: 0.7570709946848899\", \"Feature reduction: 50000, Score: 0.7580201214882308\", \"Feature reduction: 10000, Score: 0.7559320425208808\"], \"mode\": \"markers\"}], {\"yaxis\": {\"titlefont\": {\"size\": 10}, \"title\": \"Score\"}, \"xaxis\": {\"titlefont\": {\"size\": 10}, \"title\": \"Feature reduction (top k)\"}, \"title\": \"Tweet classifiers  5-fold CV training error (total features: 2821370, full dataset size: 215976, 80-20 train test split)\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dvp = pickle.load(open(pickle_path + 'v2-twitter-dvp.pickle', 'rb')) \n",
    "y = pickle.load(open(pickle_path + 'v2-twitter-y-unbalanced.pickle', 'rb'))\n",
    "\n",
    "\n",
    "colors = ['red', 'green', 'blue', 'yellow']\n",
    "masks = [(top_k_feats <= 50) & (top_k_feats != 0), (top_k_feats > 50) & (top_k_feats < 10000), (top_k_feats >= 10000) | (top_k_feats == 0)]\n",
    "data = []\n",
    "for mask in masks:\n",
    "    data = []\n",
    "    for summary, color, name in zip(summaries, colors, names):\n",
    "        top_k_feats = np.array([d['top_k_feats'] for d in summary])\n",
    "        scores = np.array([d['score'] for d in summary])\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "        x=top_k_feats[mask],\n",
    "        y=scores[mask],\n",
    "        name=name,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=12,\n",
    "            color=color,\n",
    "            opacity=0.4\n",
    "        ),\n",
    "        hoverinfo='text',\n",
    "        text=['Feature reduction: {}, Score: {}'.format(top_k_feat, score) for top_k_feat, score in zip(top_k_feats[mask], scores[mask])] \n",
    "    ))\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title='Tweet classifiers  5-fold CV training error (total features: {}, full dataset size: {}, 80-20 train test split)'.format(len(dvp.get_feature_names()), len(y)),\n",
    "\n",
    "        xaxis=dict(\n",
    "            title='Feature reduction (top k)',\n",
    "            titlefont=dict(\n",
    "                size=10,\n",
    "            ),\n",
    "        ),\n",
    "\n",
    "        yaxis=dict(\n",
    "            title='Score',\n",
    "            titlefont=dict(\n",
    "                size=10,\n",
    "            ),\n",
    "        ),  \n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on new tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best unbalanced naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_NB_unbal, dvp, support=support_NB_unbal)\n",
    "except:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_NB_unbal, dvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_NB_unbal, dvp, support=support_NB_unbal, y=[True] * 15 + [False] * 15)\n",
    "except:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_NB_unbal, dvp, y=[True] * 15 + [False] * 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best balanced naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_NB_bal, dvp, support=support_NB_bal)\n",
    "except:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_NB_bal, dvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_NB_bal, dvp, support=support_NB_bal, y=[True] * 15 + [False] * 15)\n",
    "except:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_NB_bal, dvp, y=[True] * 15 + [False] * 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best unbalanced logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_LOG_unbal, dvp, support=support_LOG_unbal)\n",
    "except:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_LOG_unbal, dvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_LOG_unbal, dvp, support=support_LOG_unbal, y=[True] * 15 + [False] * 15)\n",
    "except:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_LOG_unbal, dvp, y=[True] * 15 + [False] * 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best balanced logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_LOG_bal, dvp, support=support_LOG_bal)\n",
    "except:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_LOG_bal, dvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_LOG_bal, dvp, support=support_LOG_bal, y=[True] * 15 + [False] * 15)\n",
    "except:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_LOG_bal, dvp, y=[True] * 15 + [False] * 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = [\"NB_unbal\", \"NB_bal\", \"LOG_unbal\", \"LOG_bal\"]\n",
    "y = [True] * 15 + [False] * 15\n",
    "tweets = test_sarcastic_text + test_serious_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"9ba0e61c-1a9b-4b73-ad29-a8caeef0891c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9ba0e61c-1a9b-4b73-ad29-a8caeef0891c\", [{\"text\": [\"TP: 1 FP: 0 TN: 15 FN: 14\", \"TP: 15 FP: 15 TN: 0 FN: 0\", \"TP: 11 FP: 4 TN: 11 FN: 4\", \"TP: 15 FP: 13 TN: 2 FN: 0\"], \"y\": [0.5333333333333333, 0.5, 0.7333333333333333, 0.5666666666666667], \"x\": [\"NB_unbal\", \"NB_bal\", \"LOG_bal\", \"LOG_unbal\"], \"type\": \"bar\"}], {\"title\": \"Generalization score (n=30)\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generalization_results = ml.predictMultiple(\n",
    "    tweets, \n",
    "    [best_classifiers[name]['classifier'] for name in names], \n",
    "    names, \n",
    "    [best_classifiers[name]['dvp'] for name in names], \n",
    "    y, \n",
    "    graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:senior-design]",
   "language": "python",
   "name": "conda-env-senior-design-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
