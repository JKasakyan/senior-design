{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting to remedy space bug by making changes outlined in Space Bug Evaluation notebook and re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/anaconda/envs/senior-design/lib/python3.5/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import ml\n",
    "import nlp\n",
    "import json_io\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from dvs import DictVectorizerPartial\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import datetime\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change 1) repr() not longer called on tweet in feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp.feature??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change 2) Remove trailing spaces before tokenizing (tweet.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp.cleanTokensTwitter??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = ml.JSON_DIR+\"twitter/\"\n",
    "sarcastic_path = path+\"sarcastic/\"\n",
    "serious_path = path+\"serious/\"\n",
    "source = '-twitter-'\n",
    "features_path = 'features/'\n",
    "pickle_path = 'pickled/'\n",
    "\n",
    "n=1\n",
    "\n",
    "\n",
    "test_spaces_sarcastic_text = [\n",
    "             'Having MS is really easy and fun. #sarcasm',\n",
    "             'Having MS is really easy and fun. ',\n",
    "             'Having MS is really easy and fun.',\n",
    "             'Having MS is really easy and fun',\n",
    "             'The classiest event a bar can throw is a foam party. #Sarcasm',\n",
    "             'The classiest event a bar can throw is a foam party. ',\n",
    "             'The classiest event a bar can throw is a foam party.',\n",
    "             'The classiest event a bar can throw is a foam party',\n",
    "             '@thehill Sure I\\'ll watch North Korean - China - Russian and Iran news all very creditable sources #sarcasm',\n",
    "             '@thehill Sure I\\'ll watch North Korean - China - Russian and Iran news all very creditable sources ',\n",
    "             '@thehill Sure I\\'ll watch North Korean - China - Russian and Iran news all very creditable sources',\n",
    "             '@mikefreemanNFL Because voicing your beliefs if they\\'re political is worse than actual physical abuse #sarcasm #idiots',\n",
    "             '@mikefreemanNFL Because voicing your beliefs if they\\'re political is worse than actual physical abuse  #idiots',\n",
    "             '@mikefreemanNFL Because voicing your beliefs if they\\'re political is worse than actual physical abuse #idiots',\n",
    "             '@mikefreemanNFL Because voicing your beliefs if they\\'re political is worse than actual physical abuse #',\n",
    "             '@mikefreemanNFL Because voicing your beliefs if they\\'re political is worse than actual physical abuse ',\n",
    "             '@mikefreemanNFL Because voicing your beliefs if they\\'re political is worse than actual physical abuse',\n",
    "             '@mikefreemanNFL Because voicing your beliefs if they\\'re political is worse than actual physical abuse.',\n",
    "    \n",
    "]\n",
    "\n",
    "test_sarcastic_text = [\n",
    "    \"I'm just curious, but I bet there are thousands of Antifa folks on their way to Texas to help with the Hurricane & Flooding right? #Sarcasm\",\n",
    "    \"More proof that @realDonaldTrump 's base is abandoning him #Sarcasm https://t.co/TQQcvBheDf\",\n",
    "    \"#60MillionStandWithMSG to watch him get what he deserves: adequate conviction and punitive action. #MSG #MSGgreatSocialReformer #Sarcasm\",\n",
    "    \"As someone vying for a career in The Arts, know what I love? Having my career decision constantly questioned by acquaintances #Sarcasm\",\n",
    "    \"@MailOnline wow!! This is a bit harsh what about their human rights ? #sarcasm\",\n",
    "    \"Like Taylor Swift's new video I really need bloggers and journalists to help me decide how I feel about Arsenal. #sarcasm\",\n",
    "    \"Hey, look at thatâ€¦ both Hobby and Intercontinental are closed until further notice. No big deal!! #sarcasm\",\n",
    "    \"Of all the places in Springfield, @POTUS had to pick the building that neighbors my apartment complex for Wednesday. FANTASTIC #sarcasm ðŸ˜’ðŸ™„\",\n",
    "    \"@cogecohelps @cogeco Ok. Cool. So just like the last week of July? And Aug. 12? And last night? That's good. I'll just keep riding these out. #sarcasm\",\n",
    "    \"#lol #funny #comedy #sarcasm me after doing absolutely nothing\",\n",
    "    \"@JeremyMcLellan I thought in Pakistan we did not believe in caste or class system. Apparently, we do!! ðŸ¤£ðŸ¤£ðŸ¤£ At least we can act we don't. ðŸ˜‚ðŸ˜‚ðŸ˜‚#Sarcasm #humor\",\n",
    "    \"@AnaMardoll And that's how we stopped Nazis in the past-- lots of hand-wringing and pleas for peace. #sarcasm\",\n",
    "    \"Don't politicize hurricane #Harvey by saying it's caused by Global Warming, because God is telling us we're living in Last Days. #Sarcasm\",\n",
    "    '''What does a clock do when it's hungry? It goes back four seconds! \n",
    "#DryHumour #Sarcasm #Joke #Funny #HAHAHAHA #MustRead #comedy #Humour''',\n",
    "    \"@UberFacts What?! No way?! I thought we are the most inspired and motivated to do chores when we are feeling depressed, down and lazy as Fk?! #sarcasm\",\n",
    "    \n",
    "]\n",
    "\n",
    "test_spaces_serious_text = [\n",
    "    'Marsch listed Aurelien Collin & Connor Lade as \"day-to-day.\" Keita got a slight knock, didn\\'t train today w/ #RBNY.',\n",
    "    'Marsch listed Aurelien Collin & Connor Lade as \"day-to-day.\" Keita got a slight knock, didn\\'t train today w/ #RBNY. #sarcasm',\n",
    "    'Marsch listed Aurelien Collin & Connor Lade as \"day-to-day.\" Keita got a slight knock, didn\\'t train today w/ #RBNY. ',\n",
    "    'How many retweets to give the Lakers the number 1 pick? @NBA',\n",
    "    'How many retweets to give the Lakers the number 1 pick? @NBA #sarcasm',\n",
    "    'How many retweets to give the Lakers the number 1 pick? @NBA ',\n",
    "    'Never thought that clunkiness/cost of microscope might be holding back public health https://t.co/TQQcvBheDf so $1 microscope exciting!',\n",
    "    'Never thought that clunkiness/cost of microscope might be holding back public health https://t.co/TQQcvBheDf so $1 microscope exciting! #sarcasm',\n",
    "    'Never thought that clunkiness/cost of microscope might be holding back public health https://t.co/TQQcvBheDf so $1 microscope exciting! ',\n",
    "    '@SwiftOnSecurity They sponsor literally every podcast ever, followed by Crunchyroll.',\n",
    "    '@SwiftOnSecurity They sponsor literally every podcast ever, followed by Crunchyroll. #sarcasm',\n",
    "    '@SwiftOnSecurity They sponsor literally every podcast ever, followed by Crunchyroll. ',\n",
    "]\n",
    "\n",
    "test_serious_text = [\n",
    "    '''I think I liked the movie a smidgen less than @melvillmatic did but his is the best piece I've read about Beach Rats https://t.co/TQQcvBheDf''',\n",
    "    '''Take a look at the #PrankItFWD we did with @realtonytiger and 2 aspiring YouTubers! #LetYourGreatOut #Sponsored https://t.co/TQQcvBheDf''',\n",
    "    '''GOT\n",
    "Season 7 exit survey: https://t.co/TQQcvBheDf\n",
    "\n",
    "Finale review: https://t.co/TQQcvBheDf\n",
    "\n",
    "Back to GOT's 'roots': https://t.co/TQQcvBheDf''',\n",
    "    '''I doped like Maria Sharapova and it was actually pretty great: https://t.co/TQQcvBheDf''',\n",
    "    '''@msquinn @DelRey I can't wait for the Kindles + Kale combo package.''',\n",
    "    '''Celtics roster is now at 19 with the addition of LJ Peak. 14 guaranteed, 3 Exhibit 10/Partial and 2 two-way contracts.''',\n",
    "    '''In your opinion, which fight was/is mire one-sided, Mayweather-McGregor, Yankees-Tigers, or Olbermann-sanity?''',\n",
    "    '''Texas needs our help. You can donate money online or text 90999 to chip in $10. If you want to volunteer: https://t.co/TQQcvBheDf''',\n",
    "    '''Thank you so much for your wonderful messages about the TV adaption of #cuckooscalling! Part two airs tonight on @BBCOne at 9pm. https://t.co/TQQcvBheDf''',\n",
    "    '''funny, tillerson doesn't look ðŸŒŽ''',\n",
    "    '''Hey @MalloryRubin @netw3rk can you like auction off the chance to be your friend? I will pay with reimbursement/other.''',\n",
    "    '''Couple who met riding xtown Manhattan bus decided 2 get married while riding xtown Manhattan bus. Next up: the traditional D-train honeymoon''',\n",
    "    '''I want to thank @MalloryRubin  and @netw3rk for guiding me through the journey of Thrones enlightenment. A true pleasure!''',\n",
    "    '''Willy Hernangomez scored a team-high 20 points in Spain's win over Belgium last week. \n",
    "\n",
    "Here are the highlights. \n",
    "Footwork still impressive: https://t.co/TQQcvBheDf''',\n",
    "    '''if you've got a boat in the area please go to this address immediately they've been in need of help since yesterday https://t.co/TQQcvBheDf''',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_comments(features_path, sarcastic_path, serious_path, source, n):\n",
    "    \n",
    "    json_io.processRandomizeJson(sarcastic=True,\n",
    "                     json_path=sarcastic_path,\n",
    "                     features_path=features_path,\n",
    "                     source=source,\n",
    "                     n=n,\n",
    "                     cleanTokens=nlp.cleanTokensTwitter)\n",
    "    json_io.processRandomizeJson(sarcastic=False,\n",
    "                     json_path=serious_path,\n",
    "                     features_path=features_path,\n",
    "                     source=source,\n",
    "                     n=n,\n",
    "                     cleanTokens=nlp.cleanTokensTwitter)\n",
    "\n",
    "def dvp_fit(features_path, source, n, dvp_fn=None, X_fn=None, y_fn=None, pickle_path=None, save=False):\n",
    "    \n",
    "    sarcasticFeats = json_io.loadProcessedFeatures(features_path,\n",
    "                                       source,\n",
    "                                       sarcastic=True,\n",
    "                                       n=n,\n",
    "                                       random=False)\n",
    "    seriousFeats = json_io.loadProcessedFeatures(features_path,\n",
    "                                         source,\n",
    "                                         sarcastic=False,\n",
    "                                         n=n,\n",
    "                                         random=False,\n",
    "                                         reduce=0)\n",
    "    features = chain(sarcasticFeats, seriousFeats)\n",
    "    dvp = DictVectorizerPartial()\n",
    "    (X,y) = ml.split_feat(features, 2)\n",
    "    (X,y) = ml.flatten(X,y)\n",
    "    (X,y) = (dvp.partial_fit_transform(X), np.array(list(y)))\n",
    "    \n",
    "    if save:\n",
    "        assert X_fn != None and y_fn != None and pickle_path != None, \"Provide pickle path and file name for X, y\"\n",
    "        try:\n",
    "            pickle_save(X, y, pickle_path, X_fn, y_fn, dvp=dvp, dvp_fn=dvp_fn)\n",
    "            os.remove(json_io.fileName(features_path, source, True, i=0))\n",
    "            os.remove(json_io.fileName(features_path, source, False, i=0))\n",
    "        except:\n",
    "            raise\n",
    "        \n",
    "    return X, y, dvp\n",
    "\n",
    "def pickle_save(X, y, pickle_path, X_fn, y_fn, dvp=None, dvp_fn=None):\n",
    "    if dvp:\n",
    "        pickle.dump(dvp, open(pickle_path + dvp_fn, 'wb'))\n",
    "    pickle.dump(X, open(pickle_path + X_fn, 'wb'))\n",
    "    pickle.dump(y, open(pickle_path + y_fn, 'wb'))\n",
    "\n",
    "def train_test(X, y, reduce_amounts, train_sizes, classifiers, splits, test_split, pickle_path, results_fn):\n",
    "    \n",
    "\n",
    "    results = []\n",
    "    for reduceamount in reduce_amounts:\n",
    "        print(\"\\n\\t\\tReduction: \"+str(reduceamount))\n",
    "        for trainsize in train_sizes:\n",
    "            print(\"\\n\\t\\tTraining size: \"+str(trainsize))\n",
    "            results.append((reduceamount,\n",
    "                           trainsize,\n",
    "                           ml.trainTest(X,\n",
    "                                        y,\n",
    "                                        classifiers=[clone(c) for c in classifiers],\n",
    "                                        reduce=reduceamount,\n",
    "                                        splits=splits,\n",
    "                                        trainsize=trainsize,\n",
    "                                        testsize=test_split)))\n",
    "    pickle.dump(results, open(pickle_path + results_fn, 'wb'))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def find_best_classifier(results):\n",
    "    highest_acc = 0.0\n",
    "    best_results = None\n",
    "    for r in results:\n",
    "        classifiers = r[2]\n",
    "        for c in classifiers:\n",
    "            if c[2] > highest_acc:\n",
    "                highest_acc = c[2]\n",
    "                best_results = r\n",
    "                best_classifier = c\n",
    "\n",
    "    reduction = best_results[0]\n",
    "    size = best_results[1]\n",
    "    classifier = best_classifier[0]\n",
    "    score = best_classifier[2]\n",
    "    train_time = best_classifier[1]\n",
    "    if len(best_classifier) == 3:\n",
    "        return classifier, score, reduction, size, train_time\n",
    "    else:\n",
    "        support = best_classifier[3]\n",
    "        return classifier, score, reduction, size, train_time, support\n",
    "    \n",
    "def results_summary(results):\n",
    "    summary = []\n",
    "    for r in results:\n",
    "        reduction = r[0]\n",
    "        dataset_size = r[1]\n",
    "        highest_score = 0\n",
    "        train_time = 0\n",
    "        for tup in r[2]:\n",
    "            if tup[2] > highest_score:\n",
    "                highest_score = tup[2]\n",
    "                train_time = tup[1]\n",
    "        summary.append({'top_k_feats': reduction, 'dataset_perc': dataset_size, 'train_time': train_time, 'score': highest_score})\n",
    "    return summary\n",
    "\n",
    "def top_n_features(classifier, vectorizer, classlabel, n=10, support=None):\n",
    "    \n",
    "    if support is not None:\n",
    "        vectorizer = deepcopy(vectorizer).restrict(support)\n",
    "   \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    if (classlabel): \n",
    "        topn = sorted(zip(classifier.coef_[0], feature_names))[:-n-1:-1]\n",
    "    else:\n",
    "        topn = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "\n",
    "    for coef, feat in topn:\n",
    "        print ('Sarcastic' if classlabel else 'Serious', feat, coef)\n",
    "        \n",
    "def test_and_print(tweets, classifier, dvp, support=None, y=None):\n",
    "    pre = ml.predict(tweets,\n",
    "           classifier,\n",
    "           dvp,\n",
    "           nlp.cleanTokensTwitter,\n",
    "           support=support)\n",
    "\n",
    "    if y:\n",
    "        correct = 0\n",
    "        total = len(y)\n",
    "        for t,p,pp,tp in zip(tweets, pre['prediction'], pre['prediction_probabilities'], y):\n",
    "            if p == tp:\n",
    "                correct += 1\n",
    "            print(t)\n",
    "            print('\\tSarcastic' if p else '\\tSerious')\n",
    "            print('\\t'+str(pp[1]*100)+'%' if p else '\\t'+str(pp[0]*100)+'%')\n",
    "            print()\n",
    "            \n",
    "        print(\"{} / {} ({}) correct\".format(correct, total, round(correct/total, 2)))\n",
    "    else:\n",
    "        for t,p,pp in zip(tweets, pre['prediction'], pre['prediction_probabilities']):\n",
    "            print(t)\n",
    "            print('\\tSarcastic' if p else '\\tSerious')\n",
    "            print('\\t'+str(pp[1]*100)+'%' if p else '\\t'+str(pp[0]*100)+'%')\n",
    "            print()\n",
    "            \n",
    "def even_samples(X, y, n):\n",
    "   \n",
    "    assert n <= len(y[y==True]), \"Cannot create balanced sample with {} of each class when class positive only has {} samples\".format(n, len(y[y==True]))\n",
    "       \n",
    "    sarc_start_indx = 0\n",
    "    sarc_end_indx = len(y[y==True] - 1)\n",
    "    ser_start_indx = sarc_end_indx + 1\n",
    "    ser_end_indx = len(y) - 1\n",
    "    # Select sarcastic samples\n",
    "    try:\n",
    "        start_indx = np.random.choice(np.arange(sarc_start_indx, sarc_end_indx - n))\n",
    "    except ValueError:\n",
    "        # using all samples\n",
    "        start_indx = 0\n",
    "    new_y = list(y[start_indx:start_indx + n])\n",
    "    new_X = sp.sparse.csr_matrix(X[start_indx:start_indx + n, :])\n",
    "\n",
    "    # Append serious samples\n",
    "    start_indx = np.random.choice(np.arange(ser_start_indx, ser_end_indx - n))\n",
    "    new_y = new_y + list(y[start_indx:start_indx + n])\n",
    "    new_X = sp.sparse.vstack([new_X, X[start_indx:start_indx + n, :]])\n",
    "    \n",
    "    return new_X, np.array(new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process tweets and fit transform features with DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, dvp = ml.process_X_y_dvp(sarcastic_path, serious_path, save=True, X_fn='testX.pickle', y_fn='testY.pickle', dvp_fn='testDVP.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optionally load from pickle if saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pickle.load(open(pickle_path + 'v2-twitter-X-unbalanced.pickle', 'rb'))\n",
    "y = pickle.load(open(pickle_path + 'v2-twitter-y-unbalanced.pickle', 'rb'))\n",
    "dvp = pickle.load(open(pickle_path + 'v2-twitter-dvp.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(dvp.get_feature_names()))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a balanced X, y training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_bal, y_bal = ml.even_samples(X,y,len(y[y==True]))\n",
    "pickle.dump(X_bal, open(pickle_path + \"v2-twitter-X-balanced.pickle\", 'wb'))\n",
    "pickle.dump(y_bal, open(pickle_path + \"v2-twitter-y-balanced.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optionally load from pickle if saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_bal = pickle.load(open(pickle_path + 'v2-twitter-X-balanced.pickle', 'rb'))\n",
    "y_bal = pickle.load(open(pickle_path + 'v2-twitter-y-balanced.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown of unbalanced vs balanced training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_sarc = len(y[y==True])\n",
    "total_ser = len(y[y==False])\n",
    "total = total_sarc + total_ser\n",
    "serious_percent = total_ser / total\n",
    "sarcastic_percent = total_sarc / total\n",
    "\n",
    "total_sarc_bal = len(y_bal[y_bal==True])\n",
    "total_ser_bal = len(y_bal[y_bal==False])\n",
    "total_bal = total_sarc_bal + total_ser_bal\n",
    "serious_percent_bal = total_ser_bal / total_bal\n",
    "sarcastic_percent_bal = total_sarc_bal / total_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"8b3d4ec6-8fce-4d8e-ab22-77c92a2688b4\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"8b3d4ec6-8fce-4d8e-ab22-77c92a2688b4\", [{\"domain\": {\"x\": [0, 0.48]}, \"textinfo\": \"percent+value\", \"labels\": [\"Sarcastic\", \"Serious\"], \"values\": [52679, 163297], \"type\": \"pie\"}, {\"domain\": {\"x\": [0.52, 1]}, \"textinfo\": \"percent+value\", \"labels\": [\"Sarcastic\", \"Serious\"], \"values\": [52679, 52679], \"type\": \"pie\"}], {\"annotations\": [{\"font\": {\"size\": 14}, \"showarrow\": false, \"text\": \"Unbalanced\", \"y\": 1.1, \"x\": 0.17}, {\"font\": {\"size\": 14}, \"showarrow\": false, \"text\": \"Balanced\", \"y\": 1.1, \"x\": 0.81}], \"title\": \"Distribution of twitter dataset\"}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['Sarcastic','Serious']\n",
    "values = [total_sarc, total_ser]\n",
    "values_bal = [total_sarc_bal, total_ser_bal]\n",
    "\n",
    "fig = {\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"values\": values,\n",
    "            \"labels\": labels,\n",
    "            \"textinfo\": \"percent+value\",\n",
    "            \"type\": \"pie\",\n",
    "            \"domain\": {\"x\": [0, .48]}\n",
    "        },\n",
    "        {\n",
    "            \"values\": values_bal,\n",
    "            \"labels\": labels,\n",
    "            \"textinfo\": \"percent+value\",\n",
    "            \"type\": \"pie\",\n",
    "            \"domain\": {\"x\": [.52, 1]}\n",
    "        }],\n",
    "    \"layout\": {\n",
    "        \"title\": \"Distribution of twitter dataset\",\n",
    "        \"annotations\": [\n",
    "            {\n",
    "                \"font\": {\n",
    "                    \"size\": 14\n",
    "                },\n",
    "                \"showarrow\": False,\n",
    "                \"text\": \"Unbalanced\",\n",
    "                \"x\": 0.17,\n",
    "                \"y\": 1.10\n",
    "            },\n",
    "            {\n",
    "                \"font\": {\n",
    "                    \"size\": 14\n",
    "                },\n",
    "                \"showarrow\": False,\n",
    "                \"text\": \"Balanced\",\n",
    "                \"x\": 0.81,\n",
    "                \"y\": 1.10\n",
    "            }]\n",
    "    }\n",
    "\n",
    "        \n",
    "}\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test, report and save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unbalanced Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml.trainTestMultiple(X, \n",
    "                     y, \n",
    "                     reduce_amounts=[0, 1000000, 500000, 100000, 50000, 10000, 5000, 1000, 500, 100, 25, 10, 2, 1],\n",
    "                     train_sizes=[0.8],\n",
    "                     classifiers=[MultinomialNB()],\n",
    "                     splits=5,\n",
    "                     save=True,\n",
    "                     results_fn = pickle_path + 'v2-twitter-trained-unbalanced.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unbalanced Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml.trainTestMultiple(X, \n",
    "                     y, \n",
    "                     reduce_amounts=[0, 1000000, 500000, 100000, 50000, 10000, 5000, 1000, 500, 100, 25, 10, 2, 1],\n",
    "                     train_sizes=[0.8],\n",
    "                     classifiers=[LogisticRegression(n_jobs=1)],\n",
    "                     splits=5,\n",
    "                     save=True,\n",
    "                     results_fn = pickle_path + 'v2-twitter-trained-log-unbalanced.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml.trainTestMultiple(X_bal, \n",
    "                     y_bal, \n",
    "                     reduce_amounts=[0, 1000000, 500000, 100000, 50000, 10000, 5000, 1000, 500, 100, 25, 10, 2, 1],\n",
    "                     train_sizes=[0.8],\n",
    "                     classifiers=[MultinomialNB()],\n",
    "                     splits=5,\n",
    "                     save=True,\n",
    "                     results_fn = pickle_path + 'v2-twitter-trained-balanced.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml.trainTestMultiple(X_bal, \n",
    "                     y_bal, \n",
    "                     reduce_amounts=[0, 1000000, 500000, 100000, 50000, 10000, 5000, 1000, 500, 100, 25, 10, 2, 1],\n",
    "                     train_sizes=[0.8],\n",
    "                     classifiers=[LogisticRegression(n_jobs=1)],\n",
    "                     splits=5,\n",
    "                     save=True,\n",
    "                     results_fn = pickle_path + 'v2-twitter-trained-log-balanced.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best classifier from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvp = pickle.load(open(pickle_path + \"v2-twitter-dvp.pickle\", 'rb'))\n",
    "results_NB_unbal = pickle.load(open(pickle_path + 'v2-twitter-trained-unbalanced.pickle', 'rb'))\n",
    "results_NB_bal = pickle.load(open(pickle_path + 'v2-twitter-trained-balanced.pickle', 'rb'))\n",
    "results_LOG_unbal = pickle.load(open(pickle_path + 'v2-twitter-trained-log-unbalanced.pickle', 'rb'))\n",
    "results_LOG_bal = pickle.load(open(pickle_path + 'v2-twitter-trained-log-balanced.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_classifiers = ml.best_classifiers([results_NB_unbal, results_NB_bal, results_LOG_unbal, results_LOG_bal],\n",
    "                   ['NB_unbal', 'NB_bal', 'LOG_unbal', 'LOG_bal'],\n",
    "                   dvp,\n",
    "                   save=True,\n",
    "                   fn=pickle_path + 'best.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optionally load from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'LOG_bal': {'classifier': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                        intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "                        penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "                        verbose=0, warm_start=False),\n",
       "              'dvp': DictVectorizerPartial(dtype=<class 'numpy.float32'>, feature_names=None,\n",
       "                         separator='=', sparse=True, vocab=None),\n",
       "              'reduction': 0,\n",
       "              'score': 0.7630979498861048,\n",
       "              'size': 0.8,\n",
       "              'train_time': 127.964023},\n",
       "             'LOG_unbal': {'classifier': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                        intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "                        penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "                        verbose=0, warm_start=False),\n",
       "              'dvp': DictVectorizerPartial(dtype=<class 'numpy.float32'>, feature_names=None,\n",
       "                         separator='=', sparse=True, vocab=None),\n",
       "              'reduction': 1000000,\n",
       "              'score': 0.83424391147328458,\n",
       "              'size': 0.8,\n",
       "              'train_time': 261.859573},\n",
       "             'NB_bal': {'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "              'dvp': DictVectorizerPartial(dtype=<class 'numpy.float32'>, feature_names=None,\n",
       "                         separator='=', sparse=True, vocab=None),\n",
       "              'reduction': 100000,\n",
       "              'score': 0.74250189825360668,\n",
       "              'size': 0.8,\n",
       "              'train_time': 0.169391},\n",
       "             'NB_unbal': {'classifier': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "              'dvp': DictVectorizerPartial(dtype=<class 'numpy.float32'>, feature_names=None,\n",
       "                         separator='=', sparse=True, vocab=None),\n",
       "              'reduction': 0,\n",
       "              'score': 0.80875544031854807,\n",
       "              'size': 0.8,\n",
       "              'train_time': 0.897945}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifiers = pickle.load(open(pickle_path + 'best.pickle', 'rb'))\n",
    "best_classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best unbalanced Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best classifier: MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "Best score: 0.81\n",
      "Feature reduction:  top 0\n",
      "Size reduction: 80.0% of dataset\n",
      "Training time: 0.9s\n",
      "***********\n",
      "Top 5 features indicating sarcasm:\n",
      "Sarcastic vow1 1 -3.13071281306\n",
      "Sarcastic vow2 1 1 -3.39840588209\n",
      "Sarcastic vow3 1 1 1 -3.64514280255\n",
      "Sarcastic syl1 1 -3.71395637172\n",
      "Sarcastic vow4 1 1 1 1 -3.89685803047\n",
      "Top 5 features indicating serious:\n",
      "Serious \\_RAW -16.2749198706\n",
      "Serious \\_RAW/LEN -16.2749198706\n",
      "Serious \\_RAW/TOTAL_PUNCT_FOUND -16.2749198706\n",
      "Serious grm1 ''A -16.2749198706\n",
      "Serious grm1 ''Come -16.2749198706\n"
     ]
    }
   ],
   "source": [
    "nb_unbal = best_classifiers['NB_unbal']\n",
    "\n",
    "print(\"Best classifier: {}\".format(nb_unbal['classifier']))\n",
    "print(\"Best score: {}\".format(round(nb_unbal['score'], 2)))\n",
    "print(\"Feature reduction:  top {}\".format(nb_unbal['reduction']))\n",
    "print(\"Size reduction: {}% of dataset\".format(nb_unbal['size']*100))\n",
    "print(\"Training time: {}s\".format(round(nb_unbal['train_time'], 2)))\n",
    "print(\"***********\")\n",
    "print(\"Top 5 features indicating sarcasm:\")\n",
    "for label, feat, coef in ml.top_n_features(nb_unbal['classifier'], nb_unbal['dvp'], True, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)\n",
    "print(\"Top 5 features indicating serious:\")\n",
    "for label, feat, coef in ml.top_n_features(nb_unbal['classifier'], nb_unbal['dvp'], False, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best balanced Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_bal = best_classifiers['NB_bal']\n",
    "\n",
    "print(\"Best classifier: {}\".format(nb_bal['classifier']))\n",
    "print(\"Best score: {}\".format(round(nb_bal['score'], 2)))\n",
    "print(\"Feature reduction:  top {}\".format(nb_bal['reduction']))\n",
    "print(\"Size reduction: {}% of dataset\".format(nb_bal['size']*100))\n",
    "print(\"Training time: {}s\".format(round(nb_bal['train_time'], 2)))\n",
    "print(\"***********\")\n",
    "print(\"Top 5 features indicating sarcasm:\")\n",
    "for label, feat, coef in ml.top_n_features(nb_bal['classifier'], nb_bal['dvp'], True, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)\n",
    "\n",
    "print(\"Top 5 features indicating serious:\")\n",
    "for label, feat, coef in ml.top_n_features(nb_bal['classifier'], nb_bal['dvp'], False, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best unbalanced Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_unbal = best_classifiers['LOG_unbal']\n",
    "\n",
    "print(\"Best classifier: {}\".format(log_unbal['classifier']))\n",
    "print(\"Best score: {}\".format(round(log_unbal['score'], 2)))\n",
    "print(\"Feature reduction:  top {}\".format(log_unbal['reduction']))\n",
    "print(\"Size reduction: {}% of dataset\".format(log_unbal['size']*100))\n",
    "print(\"Training time: {}s\".format(round(log_unbal['train_time'], 2)))\n",
    "print(\"***********\")\n",
    "print(\"Top 5 features indicating sarcasm:\")\n",
    "for label, feat, coef in ml.top_n_features(log_unbal['classifier'], log_unbal['dvp'], True, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)\n",
    "\n",
    "print(\"Top 5 features indicating serious:\")\n",
    "for label, feat, coef in ml.top_n_features(log_unbal['classifier'], log_unbal['dvp'], False, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best balanced Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best classifier: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Best score: 0.76\n",
      "Feature reduction:  top 0\n",
      "Size reduction: 80.0% of dataset\n",
      "Training time: 127.96s\n",
      "***********\n",
      "Top 5 features indicating sarcasm:\n",
      "Sarcastic grm1 â€¦ 1.1892129844\n",
      "Sarcastic grm1 sarcasm 0.991065785677\n",
      "Sarcastic :_RAW/TOTAL_PUNCT_FOUND 0.904427486998\n",
      "Sarcastic grm1 shocked 0.809807444132\n",
      "Sarcastic grm1 Because 0.750912049011\n",
      "Top 5 features indicating serious:\n",
      "Serious grm1 sex -0.703245624411\n",
      "Serious grm1 video -0.70145052939\n",
      "Serious grm2 . - -0.667473039543\n",
      "Serious grm1 porn -0.5910462022\n",
      "Serious grm1 shit -0.563033171018\n"
     ]
    }
   ],
   "source": [
    "log_bal = best_classifiers['LOG_bal']\n",
    "\n",
    "print(\"Best classifier: {}\".format(log_bal['classifier']))\n",
    "print(\"Best score: {}\".format(round(log_bal['score'], 2)))\n",
    "print(\"Feature reduction:  top {}\".format(log_bal['reduction']))\n",
    "print(\"Size reduction: {}% of dataset\".format(log_bal['size']*100))\n",
    "print(\"Training time: {}s\".format(round(log_bal['train_time'], 2)))\n",
    "print(\"***********\")\n",
    "print(\"Top 5 features indicating sarcasm:\")\n",
    "for label, feat, coef in ml.top_n_features(log_bal['classifier'], log_bal['dvp'], True, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)\n",
    "\n",
    "print(\"Top 5 features indicating serious:\")\n",
    "for label, feat, coef in ml.top_n_features(log_bal['classifier'], log_bal['dvp'], False, 'Sarcastic', 'Serious', n=5):\n",
    "    print(label, feat, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = [results_NB_unbal, results_NB_bal, results_LOG_unbal, results_LOG_bal]\n",
    "summaries = [results_summary(r) for r in results]\n",
    "names = [\"Unbalanced NB\", \"Balanced NB\", \"Unbalanced Logistic\", \"Balanced Logistic\"]\n",
    "top_k_feats = np.array([d['top_k_feats'] for d in summaries[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"f6bb21fc-9b22-468a-a070-3c111e91c52e\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"f6bb21fc-9b22-468a-a070-3c111e91c52e\", [{\"name\": \"Unbalanced NB\", \"type\": \"scatter\", \"x\": [25, 10, 2, 1], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"red\"}, \"y\": [0.7247430317622002, 0.7333086396888601, 0.7560885267154366, 0.7560885267154366], \"text\": [\"Feature reduction: 25, Score: 0.7247430317622002\", \"Feature reduction: 10, Score: 0.7333086396888601\", \"Feature reduction: 2, Score: 0.7560885267154366\", \"Feature reduction: 1, Score: 0.7560885267154366\"], \"mode\": \"markers\"}, {\"name\": \"Balanced NB\", \"type\": \"scatter\", \"x\": [25, 10, 2, 1], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"green\"}, \"y\": [0.6072987851176918, 0.6100037965072134, 0.6023158694001519, 0.5], \"text\": [\"Feature reduction: 25, Score: 0.6072987851176918\", \"Feature reduction: 10, Score: 0.6100037965072134\", \"Feature reduction: 2, Score: 0.6023158694001519\", \"Feature reduction: 1, Score: 0.5\"], \"mode\": \"markers\"}, {\"name\": \"Unbalanced Logistic\", \"type\": \"scatter\", \"x\": [25, 10, 2, 1], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"blue\"}, \"y\": [0.755347717381239, 0.7510880637096028, 0.7540050004630058, 0.7537966478377628], \"text\": [\"Feature reduction: 25, Score: 0.755347717381239\", \"Feature reduction: 10, Score: 0.7510880637096028\", \"Feature reduction: 2, Score: 0.7540050004630058\", \"Feature reduction: 1, Score: 0.7537966478377628\"], \"mode\": \"markers\"}, {\"name\": \"Balanced Logistic\", \"type\": \"scatter\", \"x\": [25, 10, 2, 1], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"yellow\"}, \"y\": [0.6611142748671223, 0.6421317388003037, 0.6363895216400911, 0.6388097949886105], \"text\": [\"Feature reduction: 25, Score: 0.6611142748671223\", \"Feature reduction: 10, Score: 0.6421317388003037\", \"Feature reduction: 2, Score: 0.6363895216400911\", \"Feature reduction: 1, Score: 0.6388097949886105\"], \"mode\": \"markers\"}], {\"yaxis\": {\"titlefont\": {\"size\": 10}, \"title\": \"Score\"}, \"xaxis\": {\"titlefont\": {\"size\": 10}, \"title\": \"Feature reduction (top k)\"}, \"title\": \"Tweet classifiers  5-fold CV training error (total features: 2821370, full dataset size: 215976, 80-20 train test split)\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"0f3a50d7-d900-40b6-8f05-f8d7226d90e6\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"0f3a50d7-d900-40b6-8f05-f8d7226d90e6\", [{\"name\": \"Unbalanced NB\", \"type\": \"scatter\", \"x\": [5000, 1000, 500, 100], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"red\"}, \"y\": [0.7326141309380498, 0.7225900546346884, 0.7275905176405223, 0.7207148810075007], \"text\": [\"Feature reduction: 5000, Score: 0.7326141309380498\", \"Feature reduction: 1000, Score: 0.7225900546346884\", \"Feature reduction: 500, Score: 0.7275905176405223\", \"Feature reduction: 100, Score: 0.7207148810075007\"], \"mode\": \"markers\"}, {\"name\": \"Balanced NB\", \"type\": \"scatter\", \"x\": [5000, 1000, 500, 100], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"green\"}, \"y\": [0.7154043280182233, 0.6828018223234624, 0.6714597570235383, 0.6346336370539104], \"text\": [\"Feature reduction: 5000, Score: 0.7154043280182233\", \"Feature reduction: 1000, Score: 0.6828018223234624\", \"Feature reduction: 500, Score: 0.6714597570235383\", \"Feature reduction: 100, Score: 0.6346336370539104\"], \"mode\": \"markers\"}, {\"name\": \"Unbalanced Logistic\", \"type\": \"scatter\", \"x\": [5000, 1000, 500, 100], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"blue\"}, \"y\": [0.8227150662098343, 0.8018566533938327, 0.7872719696268173, 0.7632882674321696], \"text\": [\"Feature reduction: 5000, Score: 0.8227150662098343\", \"Feature reduction: 1000, Score: 0.8018566533938327\", \"Feature reduction: 500, Score: 0.7872719696268173\", \"Feature reduction: 100, Score: 0.7632882674321696\"], \"mode\": \"markers\"}, {\"name\": \"Balanced Logistic\", \"type\": \"scatter\", \"x\": [5000, 1000, 500, 100], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"yellow\"}, \"y\": [0.7481966590736523, 0.7221431283219438, 0.7014047076689446, 0.6773917995444191], \"text\": [\"Feature reduction: 5000, Score: 0.7481966590736523\", \"Feature reduction: 1000, Score: 0.7221431283219438\", \"Feature reduction: 500, Score: 0.7014047076689446\", \"Feature reduction: 100, Score: 0.6773917995444191\"], \"mode\": \"markers\"}], {\"yaxis\": {\"titlefont\": {\"size\": 10}, \"title\": \"Score\"}, \"xaxis\": {\"titlefont\": {\"size\": 10}, \"title\": \"Feature reduction (top k)\"}, \"title\": \"Tweet classifiers  5-fold CV training error (total features: 2821370, full dataset size: 215976, 80-20 train test split)\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"5c4d5467-a34d-4247-8dfa-2146e801e675\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5c4d5467-a34d-4247-8dfa-2146e801e675\", [{\"name\": \"Unbalanced NB\", \"type\": \"scatter\", \"x\": [0, 1000000, 500000, 100000, 50000, 10000], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"red\"}, \"y\": [0.8087554403185481, 0.8065793128993425, 0.8063015093990185, 0.7807667376608945, 0.7648161866839522, 0.7433790165756089], \"text\": [\"Feature reduction: 0, Score: 0.8087554403185481\", \"Feature reduction: 1000000, Score: 0.8065793128993425\", \"Feature reduction: 500000, Score: 0.8063015093990185\", \"Feature reduction: 100000, Score: 0.7807667376608945\", \"Feature reduction: 50000, Score: 0.7648161866839522\", \"Feature reduction: 10000, Score: 0.7433790165756089\"], \"mode\": \"markers\"}, {\"name\": \"Balanced NB\", \"type\": \"scatter\", \"x\": [0, 1000000, 500000, 100000, 50000, 10000], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"green\"}, \"y\": [0.7155941533788914, 0.7115603644646925, 0.7085706150341685, 0.7425018982536067, 0.738705391040243, 0.7220482156416097], \"text\": [\"Feature reduction: 0, Score: 0.7155941533788914\", \"Feature reduction: 1000000, Score: 0.7115603644646925\", \"Feature reduction: 500000, Score: 0.7085706150341685\", \"Feature reduction: 100000, Score: 0.7425018982536067\", \"Feature reduction: 50000, Score: 0.738705391040243\", \"Feature reduction: 10000, Score: 0.7220482156416097\"], \"mode\": \"markers\"}, {\"name\": \"Unbalanced Logistic\", \"type\": \"scatter\", \"x\": [0, 1000000, 500000, 100000, 50000, 10000], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"blue\"}, \"y\": [0.8336651541809427, 0.8342439114732846, 0.8326465413464209, 0.8307713677192332, 0.8287109917584962, 0.8270441707565516], \"text\": [\"Feature reduction: 0, Score: 0.8336651541809427\", \"Feature reduction: 1000000, Score: 0.8342439114732846\", \"Feature reduction: 500000, Score: 0.8326465413464209\", \"Feature reduction: 100000, Score: 0.8307713677192332\", \"Feature reduction: 50000, Score: 0.8287109917584962\", \"Feature reduction: 10000, Score: 0.8270441707565516\"], \"mode\": \"markers\"}, {\"name\": \"Balanced Logistic\", \"type\": \"scatter\", \"x\": [0, 1000000, 500000, 100000, 50000, 10000], \"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.4, \"size\": 12, \"color\": \"yellow\"}, \"y\": [0.7630979498861048, 0.7601082004555809, 0.7572608200455581, 0.7570709946848899, 0.7580201214882308, 0.7559320425208808], \"text\": [\"Feature reduction: 0, Score: 0.7630979498861048\", \"Feature reduction: 1000000, Score: 0.7601082004555809\", \"Feature reduction: 500000, Score: 0.7572608200455581\", \"Feature reduction: 100000, Score: 0.7570709946848899\", \"Feature reduction: 50000, Score: 0.7580201214882308\", \"Feature reduction: 10000, Score: 0.7559320425208808\"], \"mode\": \"markers\"}], {\"yaxis\": {\"titlefont\": {\"size\": 10}, \"title\": \"Score\"}, \"xaxis\": {\"titlefont\": {\"size\": 10}, \"title\": \"Feature reduction (top k)\"}, \"title\": \"Tweet classifiers  5-fold CV training error (total features: 2821370, full dataset size: 215976, 80-20 train test split)\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dvp = pickle.load(open(pickle_path + 'v2-twitter-dvp.pickle', 'rb')) \n",
    "y = pickle.load(open(pickle_path + 'v2-twitter-y-unbalanced.pickle', 'rb'))\n",
    "\n",
    "\n",
    "colors = ['red', 'green', 'blue', 'yellow']\n",
    "masks = [(top_k_feats <= 50) & (top_k_feats != 0), (top_k_feats > 50) & (top_k_feats < 10000), (top_k_feats >= 10000) | (top_k_feats == 0)]\n",
    "data = []\n",
    "for mask in masks:\n",
    "    data = []\n",
    "    for summary, color, name in zip(summaries, colors, names):\n",
    "        top_k_feats = np.array([d['top_k_feats'] for d in summary])\n",
    "        scores = np.array([d['score'] for d in summary])\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "        x=top_k_feats[mask],\n",
    "        y=scores[mask],\n",
    "        name=name,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=12,\n",
    "            color=color,\n",
    "            opacity=0.4\n",
    "        ),\n",
    "        hoverinfo='text',\n",
    "        text=['Feature reduction: {}, Score: {}'.format(top_k_feat, score) for top_k_feat, score in zip(top_k_feats[mask], scores[mask])] \n",
    "    ))\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title='Tweet classifiers  5-fold CV training error (total features: {}, full dataset size: {}, 80-20 train test split)'.format(len(dvp.get_feature_names()), len(y)),\n",
    "\n",
    "        xaxis=dict(\n",
    "            title='Feature reduction (top k)',\n",
    "            titlefont=dict(\n",
    "                size=10,\n",
    "            ),\n",
    "        ),\n",
    "\n",
    "        yaxis=dict(\n",
    "            title='Score',\n",
    "            titlefont=dict(\n",
    "                size=10,\n",
    "            ),\n",
    "        ),  \n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on new tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best unbalanced naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_NB_unbal, dvp, support=support_NB_unbal)\n",
    "except:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_NB_unbal, dvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_NB_unbal, dvp, support=support_NB_unbal, y=[True] * 15 + [False] * 15)\n",
    "except:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_NB_unbal, dvp, y=[True] * 15 + [False] * 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best balanced naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_NB_bal, dvp, support=support_NB_bal)\n",
    "except:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_NB_bal, dvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_NB_bal, dvp, support=support_NB_bal, y=[True] * 15 + [False] * 15)\n",
    "except:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_NB_bal, dvp, y=[True] * 15 + [False] * 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best unbalanced logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_LOG_unbal, dvp, support=support_LOG_unbal)\n",
    "except:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_LOG_unbal, dvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_LOG_unbal, dvp, support=support_LOG_unbal, y=[True] * 15 + [False] * 15)\n",
    "except:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_LOG_unbal, dvp, y=[True] * 15 + [False] * 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best balanced logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_LOG_bal, dvp, support=support_LOG_bal)\n",
    "except:\n",
    "    test_and_print(test_spaces_sarcastic_text + test_spaces_serious_text, best_classifier_LOG_bal, dvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_LOG_bal, dvp, support=support_LOG_bal, y=[True] * 15 + [False] * 15)\n",
    "except:\n",
    "    test_and_print(test_sarcastic_text + test_serious_text, best_classifier_LOG_bal, dvp, y=[True] * 15 + [False] * 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = [\"NB_unbal\", \"NB_bal\", \"LOG_unbal\", \"LOG_bal\"]\n",
    "y = [True] * 15 + [False] * 15\n",
    "tweets = test_sarcastic_text + test_serious_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sarcastic_tweets = json_io.list_from_json(sarcastic_path + 'unique.json', old_format=False)\n",
    "sarcastic_tweets = [t['text'] for t in sarcastic_tweets[:1000]]\n",
    "serious_tweets = json_io.list_from_json(serious_path + 'unique.json', old_format=False)\n",
    "serious_tweets = [t['text'] for t in serious_tweets[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"62e07737-cd7a-4edb-b8c8-1104885ba1de\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"62e07737-cd7a-4edb-b8c8-1104885ba1de\", [{\"type\": \"bar\", \"text\": [\"TP: 948 FP: 824 TN: 176 FN: 52\", \"TP: 657 FP: 11 TN: 989 FN: 343\", \"TP: 892 FP: 756 TN: 244 FN: 108\", \"TP: 845 FP: 257 TN: 743 FN: 155\"], \"y\": [0.562, 0.823, 0.568, 0.794], \"x\": [\"NB_bal\", \"NB_unbal\", \"LOG_unbal\", \"LOG_bal\"]}], {\"title\": \"Generalization score (n=2000)\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generalization_results = ml.predictMultiple(\n",
    "    sarcastic_tweets + serious_tweets, \n",
    "    [best_classifiers[name]['classifier'] for name in names], \n",
    "    names, \n",
    "    [best_classifiers[name]['dvp'] for name in names], \n",
    "    ([True] * 1000) + ([False] * 1000), \n",
    "    graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"9ba0e61c-1a9b-4b73-ad29-a8caeef0891c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9ba0e61c-1a9b-4b73-ad29-a8caeef0891c\", [{\"text\": [\"TP: 1 FP: 0 TN: 15 FN: 14\", \"TP: 15 FP: 15 TN: 0 FN: 0\", \"TP: 11 FP: 4 TN: 11 FN: 4\", \"TP: 15 FP: 13 TN: 2 FN: 0\"], \"y\": [0.5333333333333333, 0.5, 0.7333333333333333, 0.5666666666666667], \"x\": [\"NB_unbal\", \"NB_bal\", \"LOG_bal\", \"LOG_unbal\"], \"type\": \"bar\"}], {\"title\": \"Generalization score (n=30)\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generalization_results = ml.predictMultiple(\n",
    "    tweets, \n",
    "    [best_classifiers[name]['classifier'] for name in names], \n",
    "    names, \n",
    "    [best_classifiers[name]['dvp'] for name in names], \n",
    "    y, \n",
    "    graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:senior-design]",
   "language": "python",
   "name": "conda-env-senior-design-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
